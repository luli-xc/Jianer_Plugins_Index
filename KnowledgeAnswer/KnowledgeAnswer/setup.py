import os
import glob
import json
import math
from collections import Counter
from Hyper import Configurator
from Tools.deepseek import dsr114
Configurator.cm = Configurator.ConfigManager(Configurator.Config(file="config.json").load_from_file())

# åŠ è½½é…ç½®
def load_config():
    config_path = os.path.join(os.path.dirname(__file__), 'knowledge_config.json')
    with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)

CONFIG = load_config()

TRIGGHT_KEYWORD = "Any"
# HELP_MESSAGE = f"""

# ç”¨äºå­˜å‚¨ç”¨æˆ·ä¸Šä¸‹æ–‡
user_lists = {}

async def on_message(event, actions, Manager, Segments):
    if not hasattr(event, 'message'):
        return False
        
    user_input = str(event.message).strip() # ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
    
    # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦åœ¨è¯¢é—®ä¸æœ¬åº”ç”¨ç›¸å…³çš„é—®é¢˜
    if not is_asking_about_app(user_input):
        return False
    
    # æ£€æŸ¥å¹¶åˆ›å»ºçŸ¥è¯†åº“æ–‡ä»¶å¤¹
    knowledge_dir = "data/knowledge"
    if not os.path.exists(knowledge_dir):
        os.makedirs(knowledge_dir)
    
    # æ£€æŸ¥çŸ¥è¯†åº“æ–‡ä»¶å¤¹æ˜¯å¦ä¸ºç©º
    if not os.listdir(knowledge_dir):
        print("æé†’ï¼šçŸ¥è¯†åº“æ–‡ä»¶å¤¹ä¸ºç©ºï¼Œè¯·åœ¨ data/knowledge æ–‡ä»¶å¤¹ä¸‹æ”¾ç½® markdown æ ¼å¼çš„ .md æ–‡æ¡£å½“ä½œçŸ¥è¯†åº“")
        return False
    
    # æ£€ç´¢çŸ¥è¯†åº“ä¸­çš„ç›¸å…³ä¿¡æ¯
    relevant_info = search_knowledge_base(knowledge_dir, user_input)
    
    # å¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³çŸ¥è¯†ï¼Œåˆ™ä¸å›å¤
    if not relevant_info:
        print(f"æœªæ£€ç´¢åˆ°ä¸ç”¨æˆ·è¾“å…¥ç›¸å…³çš„çŸ¥è¯†ï¼š{user_input}")
        return False
    
    # ç»“åˆDeepSeekç”Ÿæˆå›å¤
    reply = await generate_reply_with_deepseek(user_input, relevant_info, event.user_id)
    
    # å‘é€å›å¤ç»™ç”¨æˆ·
    await actions.send(group_id=event.group_id, message=Manager.Message(Segments.Reply(event.message_id), 
                                                                        Segments.At(event.user_id), 
                                                                        Segments.Text(f" {reply.strip()}")))

    return True

def is_asking_about_app(user_input):
    """åˆ¤æ–­ç”¨æˆ·è¾“å…¥æ˜¯å¦ä¸æœ¬åº”ç”¨ç›¸å…³çš„é—®é¢˜"""
    # ä»é…ç½®ä¸­è·å–å…³é”®è¯
    app_keywords = CONFIG['keywords']['main']
    
    # ä»é…ç½®ä¸­è·å–ç–‘é—®è¯
    question_words = CONFIG['question_words']
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«åº”ç”¨ç›¸å…³å…³é”®è¯
    contains_app = any(keyword in user_input for keyword in app_keywords)
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç–‘é—®è¯
    contains_question = any(qw in user_input for qw in question_words)
    
    # æ£€æŸ¥æ˜¯å¦ä»¥ç–‘é—®è¯å¼€å¤´
    starts_with_question = user_input.startswith(tuple(question_words))
    
    # æ£€æŸ¥æ˜¯å¦ä»¥é—®å·ç»“å°¾
    ends_with_question_mark = user_input.strip().endswith(('?', 'ï¼Ÿ'))
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«åŠ©åŠ¨è¯ï¼ˆè¡¨æ˜å¯èƒ½åœ¨è¯¢é—®èƒ½åŠ›ã€å¯èƒ½æ€§ï¼‰
    modal_verbs = CONFIG['modal_verbs']
    contains_modal = any(modal in user_input for modal in modal_verbs)
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«åŠ¨ä½œè¯
    action_verbs = CONFIG['action_verbs']
    contains_action = any(action in user_input for action in action_verbs)
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹å®šé—®é¢˜æ¨¡å¼
    question_patterns = CONFIG['question_patterns']
    contains_pattern = any(pattern in user_input for pattern in question_patterns)
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºç–‘é—®å¥ï¼ˆç»¼åˆè€ƒè™‘å¤šç§å› ç´ ï¼‰
    is_question = contains_question or starts_with_question or ends_with_question_mark or contains_modal
    
    # æé«˜åˆ¤æ–­å‡†ç¡®æ€§ï¼šç¡®ä¿ç”¨æˆ·ç¡®å®æ˜¯åœ¨é—®é—®é¢˜è€Œä¸æ˜¯ç®€å•æåŠ
    # æ£€æŸ¥æ–‡æœ¬é•¿åº¦
    text_length = len(user_input.strip())
    
    # æ£€æŸ¥æ˜¯å¦åªæ˜¯ç®€å•ç§°å‘¼æˆ–æ„Ÿå¹
    simple_expressions = CONFIG['simple_expressions']
    is_simple_expression = user_input in simple_expressions
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¦å®šè¯ï¼ˆå¯èƒ½æ˜¯è´Ÿé¢è¯„ä»·è€Œéè¯¢é—®ï¼‰
    negation_words = CONFIG['negation_words']
    contains_negation = any(neg in user_input for neg in negation_words)
    
    # ç»¼åˆåˆ¤æ–­ï¼šå¿…é¡»åŒ…å«åº”ç”¨å…³é”®è¯ï¼Œæ˜¯ç–‘é—®å¥æˆ–åŒ…å«åŠ¨è¯ï¼Œä¸”ä¸æ˜¯ç®€å•ç§°å‘¼
    return (
        contains_app 
        and (is_question or contains_action or contains_pattern) 
        and not is_simple_expression 
        and text_length > CONFIG['min_text_length']  # ä»é…ç½®ä¸­è·å–æœ€å°é•¿åº¦è¦æ±‚
    )

def chinese_tokenize(text):
    """ä¸­æ–‡åˆ†è¯å‡½æ•°ï¼ˆç®€å•å®ç°ï¼‰"""
    # è½¬æ¢ä¸ºå°å†™
    text = text.lower()
    
    # ä»é…ç½®ä¸­è·å–æ ‡ç‚¹ç¬¦å·
    punctuation = CONFIG['punctuation']
    for p in punctuation:
        text = text.replace(p, " ")
    
    # ç®€å•åˆ†è¯
    words = []
    current_word = ""
    
    for char in text:
        if char.isalnum() or char == '_':
            current_word += char
        else:
            if current_word:
                words.append(current_word)
                current_word = ""
            if char.strip():
                words.append(char)
    
    if current_word:
        words.append(current_word)
    
    return words

def extract_phrases(text):
    """æå–æ–‡æœ¬ä¸­çš„çŸ­è¯­å’Œä¸“æœ‰åè¯"""
    # è½¬æ¢ä¸ºå°å†™
    text = text.lower()
    
    # ä»é…ç½®ä¸­è·å–æ ‡ç‚¹ç¬¦å·
    punctuation = CONFIG['punctuation']
    for p in punctuation:
        text = text.replace(p, " ")
    
    # æå–å¯èƒ½çš„çŸ­è¯­ï¼ˆè¿ç»­çš„æ±‰å­—æˆ–å­—æ¯æ•°å­—ç»„åˆï¼‰
    phrases = []
    current_phrase = ""
    
    for char in text:
        if char.isalnum() or char == '_':
            current_phrase += char
        else:
            if current_phrase and len(current_phrase) > 1:
                phrases.append(current_phrase)
                current_phrase = ""
    
    if current_phrase and len(current_phrase) > 1:
        phrases.append(current_phrase)
    
    # ä»é…ç½®ä¸­è·å–å¸¸è§çš„ä¸“æœ‰åè¯
    common_phrases = CONFIG['proper_nouns']
    for phrase in common_phrases:
        if phrase.lower() in text:
            phrases.append(phrase.lower())
    
    return set(phrases)

def extract_keywords(text):
    """æå–æ–‡æœ¬å…³é”®è¯"""
    # åˆ†è¯
    words = chinese_tokenize(text)
    
    # æå–çŸ­è¯­å’Œä¸“æœ‰åè¯
    phrases = extract_phrases(text)
    
    # ä»é…ç½®ä¸­è·å–åœç”¨è¯
    stop_words = set(CONFIG['stop_words'])
    
    # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
    keywords = [word for word in words if word not in stop_words and len(word) > 1]
    
    # ä»é…ç½®ä¸­è·å–åŒä¹‰è¯æ‰©å±•
    synonyms = {}
    for key, value in CONFIG['keywords'].items():
        if key != 'main':  # main æ˜¯ä¸»è¦å…³é”®è¯ï¼Œä¸éœ€è¦ä½œä¸ºåŒä¹‰è¯å¤„ç†
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªè¯ä½œä¸ºåŸºç¡€è¯ï¼Œå…¶ä½™ä½œä¸ºåŒä¹‰è¯
            if len(value) > 1:
                synonyms[value[0]] = value[1:]
    
    # æ‰©å±•å…³é”®è¯
    extended_keywords = set(keywords)
    # æ·»åŠ çŸ­è¯­å’Œä¸“æœ‰åè¯
    extended_keywords.update(phrases)
    
    # åŒä¹‰è¯æ‰©å±•
    for keyword in list(extended_keywords):
        for base_word, syns in synonyms.items():
            if keyword == base_word or keyword in syns:
                extended_keywords.update([base_word] + syns)
    
    return extended_keywords

def calculate_similarity(text1, text2, keywords1=None, keywords2=None, filename=""):
    """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
    # å¦‚æœæ²¡æœ‰æä¾›å…³é”®è¯ï¼Œæå–å…³é”®è¯
    if keywords1 is None:
        keywords1 = extract_keywords(text1)
    if keywords2 is None:
        keywords2 = extract_keywords(text2)
    
    # ç¡®ä¿keywords1å’Œkeywords2æ˜¯é›†åˆ
    keywords1 = set(keywords1) if keywords1 else set()
    keywords2 = set(keywords2) if keywords2 else set()
    
    # è®¡ç®—å…±åŒå…³é”®è¯æ•°
    common_keywords = keywords1.intersection(keywords2)
    
    # è®¡ç®—å…³é”®è¯ç›¸ä¼¼åº¦
    if len(keywords1) == 0 or len(keywords2) == 0:
        keyword_similarity = 0.0
    else:
        keyword_similarity = len(common_keywords) / math.sqrt(len(keywords1) * len(keywords2))
    
    # è®¡ç®—æ–‡æœ¬é•¿åº¦ç›¸ä¼¼åº¦ï¼ˆæƒ©ç½šè¿‡é•¿æˆ–è¿‡çŸ­çš„æ–‡æœ¬ï¼‰
    len1 = len(text1)
    len2 = len(text2)
    if len1 + len2 == 0:
        length_similarity = 1.0
    else:
        length_similarity = 1.0 - abs(len1 - len2) / (len1 + len2 + 1)
    
    # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆåŸºäºå…³é”®è¯è¦†ç›–ç‡ï¼‰
    coverage1 = len(common_keywords) / (len(keywords1) + 1)
    coverage2 = len(common_keywords) / (len(keywords2) + 1)
    coverage_similarity = (coverage1 + coverage2) / 2
    
    # ç»¼åˆç›¸ä¼¼åº¦
    similarity = 0.4 * keyword_similarity + 0.1 * length_similarity + 0.3 * coverage_similarity
    
    # é¢å¤–åŠ åˆ†ï¼šå¦‚æœæ–‡æœ¬2åŒ…å«æ–‡æœ¬1çš„ä¸»è¦è¯æ±‡
    text1_lower = text1.lower()
    text2_lower = text2.lower()
    main_words = [word for word in chinese_tokenize(text1) if len(word) > 2]
    if main_words:
        main_word_matches = sum(1 for word in main_words if word in text2_lower)
        if main_word_matches > 0:
            similarity += 0.1 * (main_word_matches / len(main_words))
    
    # é¢å¤–åŠ åˆ†ï¼šå¦‚æœæ˜¯æ–‡ä»¶ååŒ¹é…
    filename_lower = filename.lower()
    if filename:
        # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«æ–‡æœ¬1çš„å…³é”®è¯
        text1_words = chinese_tokenize(text1)
        filename_matches = sum(1 for word in text1_words if len(word) > 1 and word.lower() in filename_lower)
        if filename_matches > 0:
            similarity += 0.2 * (filename_matches / len([w for w in text1_words if len(w) > 1]))
    
    # é¢å¤–åŠ åˆ†ï¼šå¦‚æœæ–‡æœ¬åŒ…å«ç›¸å…³çš„è¯­ä¹‰è¯æ±‡
    semantic_boost = 0.0
    semantic_groups = CONFIG['semantic_groups']
    
    for group in semantic_groups:
        if any(word in text1_lower for word in group) and any(word in text2_lower for word in group):
            semantic_boost += 0.15
            break
    
    similarity += semantic_boost
    
    # é¢å¤–åŠ åˆ†ï¼šåŸºäºè¯­ä¹‰è·ç¦»
    semantic_distance_boost = 0.0
    # è®¡ç®—è¯­ä¹‰è·ç¦»ï¼ˆç®€å•å®ç°ï¼‰
    if len(common_keywords) > 0:
        semantic_distance_boost = 0.05
    
    similarity += semantic_distance_boost
    
    # é¢å¤–åŠ åˆ†ï¼šå¯¹äºç‰¹å®šå…³é”®è¯çš„åŒ¹é…
    # ä»é…ç½®ä¸­è·å–ç‰¹å®šå…³é”®è¯åŒ¹é…è§„åˆ™
    specific_keywords = {}
    for key, value in CONFIG['keywords'].items():
        if key in ['deploy', 'usage', 'permanent_plugin', 'trigger_plugin', 'bot', 'napcat', 'qq_bot']:
            specific_keywords[value[0]] = value
    
    for base_word, variants in specific_keywords.items():
        if any(variant in text1_lower for variant in variants) and any(variant in text2_lower for variant in variants):
            similarity += 0.2  # æé«˜ç‰¹å®šå…³é”®è¯çš„åŒ¹é…æƒé‡
            break
    
    # é¢å¤–åŠ åˆ†ï¼šä¸“æœ‰åè¯åŒ¹é…
    proper_nouns = CONFIG['proper_nouns']
    proper_noun_matches = sum(1 for noun in proper_nouns if noun.lower() in text1_lower and noun.lower() in text2_lower)
    if proper_noun_matches > 0:
        similarity += 0.3 * proper_noun_matches  # å¤§å¹…æé«˜ä¸“æœ‰åè¯çš„åŒ¹é…æƒé‡
    
    return min(similarity, 1.0)

def search_knowledge_base(knowledge_dir, user_input):
    """åœ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯"""
    relevant_info = []
    
    # æŸ¥æ‰¾æ‰€æœ‰markdownæ–‡ä»¶
    md_files = glob.glob(os.path.join(knowledge_dir, "*.md"))
    
    # æå–ç”¨æˆ·é—®é¢˜çš„å…³é”®è¯
    user_keywords = extract_keywords(user_input)
    
    # å¯¹æ¯ä¸ªæ–‡ä»¶è¿›è¡Œå¤„ç†
    for md_file in md_files:
        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
                
                # æå–æ–‡ä»¶å
                file_name = os.path.basename(md_file)
                
                # æå–æ–‡ä»¶å…³é”®è¯
                file_keywords = extract_keywords(content)
                
                # è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨æ”¹è¿›çš„ç®—æ³•ï¼Œä¼ é€’æ–‡ä»¶åï¼‰
                similarity = calculate_similarity(user_input, content, user_keywords, file_keywords, file_name)
                
                # æ‰“å°è°ƒè¯•ä¿¡æ¯
                print(f"æ–‡ä»¶: {file_name}, ç›¸ä¼¼åº¦: {similarity}")
                
                # ä»é…ç½®ä¸­è·å–ç›¸ä¼¼åº¦é˜ˆå€¼
                threshold = CONFIG['similarity_threshold']
                if similarity > threshold:
                    # ç›´æ¥ä½¿ç”¨æ•´ä¸ªæ–‡ä»¶å†…å®¹ï¼Œç¡®ä¿ä¸ä¸¢å¤±ä»»ä½•ä¿¡æ¯
                    relevant_info.append({
                        'file': file_name,
                        'content': content,
                        'similarity': similarity
                    })
        except Exception as e:
            print(f"å¤„ç†æ–‡ä»¶ {os.path.basename(md_file)} æ—¶å‡ºé”™: {e}")
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    relevant_info.sort(key=lambda x: x['similarity'], reverse=True)
    
    # ä»é…ç½®ä¸­è·å–æœ€å¤§ç»“æœæ•°
    max_results = CONFIG['max_results']
    # ä»é…ç½®ä¸­è·å–ç›¸ä¼¼åº¦é˜ˆå€¼
    threshold = CONFIG['similarity_threshold']
    
    # åªè¿”å›è¶…è¿‡é˜ˆå€¼ä¸”åœ¨æœ€å¤§æ•°é‡èŒƒå›´å†…çš„æœ€ç›¸å…³ç»“æœ
    filtered_results = [item for item in relevant_info if item['similarity'] > threshold]
    return filtered_results[:max_results]

async def generate_reply_with_deepseek(user_input, relevant_info, user_id):
    """ä½¿ç”¨DeepSeekç”Ÿæˆå›å¤"""
    # æ„å»ºçŸ¥è¯†åº“ä¿¡æ¯
    knowledge_info = "\n".join([f"æ–‡ä»¶: {info['file']}\nå†…å®¹: {info['content']}\n" for info in relevant_info])
    
    # ä»é…ç½®ä¸­è·å–åº”ç”¨åç§°
    app_name = CONFIG['app_name']
    
    # ä»é…ç½®ä¸­è·å–promptæ¨¡æ¿å¹¶æ ¼å¼åŒ–
    prompt_template = CONFIG['ai_prompt_template']
    prompt = prompt_template.format(app_name=app_name, knowledge_info=knowledge_info)
    
    # ä»é…ç½®ä¸­è·å–æ¶ˆæ¯æ¨¡æ¿å¹¶æ ¼å¼åŒ–
    message_template = CONFIG['ai_message_template']
    message = message_template.format(user_input=user_input)
    
    # è·å–DeepSeek APIå¯†é’¥
    deepseek_key = Configurator.cm.get_cfg().others.get("deepseek_key", None)
    if not deepseek_key:
        # å¦‚æœæ²¡æœ‰é…ç½®APIå¯†é’¥ï¼Œä½¿ç”¨é»˜è®¤å›å¤
        return generate_default_reply(user_input, relevant_info)
    
    # è°ƒç”¨DeepSeekï¼ˆå…³é—­æµå¼å“åº”ï¼‰
    try:
        ds = dsr114(prompt, message, user_lists, user_id, "deepseek-chat", app_name, deepseek_key, stream=False)
        response = ""
        for chunk, chunk_type in ds.Response():
            if isinstance(chunk, str) and chunk_type == 'message':
                response = chunk
                break
        
        # æ„å»ºä¿¡æ¯æ¥æº
        sources = "\n".join([f"- {info['file']}" for info in relevant_info])
        # ä»é…ç½®ä¸­è·å–æ¥æºæ¨¡æ¿å¹¶æ ¼å¼åŒ–
        source_template = CONFIG['source_template']
        response += source_template.format(sources=sources)
        
        return response
    except Exception as e:
        print(f"è°ƒç”¨DeepSeekæ—¶å‡ºé”™: {e}")
        # å‡ºé”™æ—¶ä½¿ç”¨é»˜è®¤å›å¤
        return generate_default_reply(user_input, relevant_info)

def generate_default_reply(user_input, relevant_info):
    """ç”Ÿæˆé»˜è®¤å›å¤"""
    # ä»é…ç½®ä¸­è·å–åº”ç”¨åç§°
    app_name = CONFIG['app_name']
    
    # æ„å»ºçŸ¥è¯†ä¿¡æ¯
    knowledge_info = ""
    for info in relevant_info:
        knowledge_info += f"ğŸ“„ **{info['file']}**\n"
        knowledge_info += f"{info['content']}\n\n"
    
    # ä»é…ç½®ä¸­è·å–é»˜è®¤å›å¤æ¨¡æ¿å¹¶æ ¼å¼åŒ–
    template = CONFIG['default_reply_template']
    reply = template.format(knowledge_info=knowledge_info, app_name=app_name)
    
    return reply